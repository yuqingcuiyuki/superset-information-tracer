{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOh7bT8+CQ+zZthEYBl4g/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouhanc/superset-information-tracer/blob/main/draft_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex8AKHbGoeAs"
      },
      "outputs": [],
      "source": [
        "!pip install mysql-connector-python\n",
        "!pip install informationtracer\n",
        "!pip install pysentimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "ljZIE7hsom5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from informationtracer import informationtracer\n",
        "import requests\n",
        "import mysql.connector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import pytz\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from pysentimiento.preprocessing import preprocess_tweet\n",
        "from pysentimiento import create_analyzer\n",
        "\n",
        "\n",
        "import itertools\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from gensim import corpora, models\n",
        "from wordcloud import WordCloud\n",
        "\n"
      ],
      "metadata": {
        "id": "GP1z1yDWtqGt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create database and table"
      ],
      "metadata": {
        "id": "JB20D0TsS8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(os.path.expanduser('db_info.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "localhost = lines[0].strip()\n",
        "username = lines[1].strip()\n",
        "pw = lines[2].strip()\n",
        "\n",
        "\n",
        "#connect to database\n",
        "mydb = mysql.connector.connect(\n",
        "  host=localhost,\n",
        "  user=username,\n",
        "  password=pw\n",
        ")\n",
        "\n",
        "\n",
        "mycursor = mydb.cursor()\n",
        "# create a databse called \"dashboard\"\n",
        "#mycursor.execute(\"CREATE DATABASE dashboard\")\n"
      ],
      "metadata": {
        "id": "qr3kqiPXoo1X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select database to modify\n",
        "mycursor.execute(\"use dashboard\")\n",
        "\n",
        "# delete table\n",
        "# mycursor.execute(\"drop table if exists infotracer\")\n",
        "# mycursor.execute(\"drop table if exists sentiment\")\n",
        "# mycursor.execute(\"drop table if exists wordcloud\")\n",
        "# mycursor.execute(\"drop table if exists network\")\n",
        "# mycursor.execute(\"drop table if exists helper\")\n",
        "\n",
        "# create table\n",
        "# mycursor.execute(\"create table infotracer (candidate_name varchar(255), text MediumTEXT, username varchar(255), num_interaction int, datetime timestamp, platform varchar(255) )\")\n",
        "# mycursor.execute(\"create table sentiment (text MediumTEXT, processed_text MediumTEXT, positive float, neutral float, negative float, label varchar(255),candidate_name varchar(255), platform varchar(255), username varchar(255), num_interaction int, datetime timestamp )\")\n",
        "# mycursor.execute(\"create table wordcloud (word varchar(255), weight float, candidate_name varchar(255),platform varchar(255))\")\n",
        "# mycursor.execute(\"create table network (source varchar(255), target varchar(255), weight float, source_cat varchar(255), target_cat varchar(255), source_datetime timestamp, target_datetime timestamp, candidate_name varchar(255))\")\n",
        "# mycursor.execute(\"create table helper (datetime timestamp,candidate_name varchar(255),label varchar(255),platform varchar(255))\")\n"
      ],
      "metadata": {
        "id": "ucamqtUBoo39"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete"
      ],
      "metadata": {
        "id": "ooRBKCvFCzRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mycursor.execute(\"DELETE FROM infotracer WHERE datetime >= '2023-04-15 18:00:00' AND datetime <= '2023-04-16 18:00:00'\")\n",
        "# mycursor.execute(\"DELETE FROM sentiment WHERE datetime >= '2023-04-15 18:00:00' AND datetime <= '2023-04-16 18:00:00'\")\n",
        "mycursor.execute(\"DELETE FROM network\")\n",
        "\n",
        "mydb.commit()"
      ],
      "metadata": {
        "id": "rFZ_pzPwCVS7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELT helper table"
      ],
      "metadata": {
        "id": "pUIB5jmQoi_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_helper_table(start_date,end_date,update_db=True):\n",
        "  \n",
        "##############################################################################\n",
        "##  This function generates a helper table that is used to join with other table to fill in time gaps.\n",
        "##  Records are inserted to table within function.\n",
        "##############################################################################\n",
        "\n",
        "  # Define platforms and labels\n",
        "  platforms = ['facebook','twitter','instagram','youtube','youtube comment']\n",
        "  labels = ['POS','NEU','NEG']\n",
        "  candidate_name=['Manolo Jiménez Salinas','Armando Guadiana Tijerina','Ricardo Mejia Berdeja','Lenin Perez Rivera']\n",
        "  # Generate date range\n",
        "  date_range = pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "  # Generate combinations of platform and label for each date\n",
        "  date_platform_label = list(itertools.product(date_range, platforms, labels,candidate_name))\n",
        "\n",
        "  # Create DataFrame\n",
        "  helper_df = pd.DataFrame(date_platform_label, columns=[\"datetime\", \"platform\", \"label\",'candidate_name'])\n",
        "\n",
        "\n",
        "  if update_db==True:\n",
        "    # Insert to db\n",
        "    helper_data = helper_df.apply(tuple, axis=1).tolist()\n",
        "    # change order of column to fit df\n",
        "    query=\"insert into helper (datetime,platform,label, candidate_name) Values(%s,%s,%s,%s);\" \n",
        "    mycursor.executemany(query,helper_data)\n",
        "    mydb.commit()\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "I7Y0EwSRgqyO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL for infotracer table"
      ],
      "metadata": {
        "id": "Ns-KU58MxAnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_time(column):\n",
        "\n",
        "##############################################################################\n",
        "##  This function takes the datetime column of any dataframe \n",
        "##  and convert utc time to mexico time.\n",
        "##  This function is part of another function. DO NOT run directly.\n",
        "##############################################################################\n",
        "\n",
        "  column=column.apply(lambda x: pytz.utc.localize(x))\n",
        "  #utc to mexico time\n",
        "  mexico_tz = timezone('America/Mexico_City')\n",
        "  convert_timestamp = lambda x: x.astimezone(mexico_tz).replace(tzinfo=None)\n",
        "  column = column.apply(convert_timestamp)\n",
        "  return column\n"
      ],
      "metadata": {
        "id": "vGiotOjzAozs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_infotracer_table(start_date,end_date,update_db=True):\n",
        "\n",
        "########################################################################################\n",
        "##  This function query data using information tracer, store in df, convert time and insert\n",
        "##  data into information tracer table. It returns a df of qury result. This result will be used for sentiment analysis.\n",
        "##  This function is part of another function. DO NOT run directly.\n",
        "########################################################################################\n",
        "\n",
        "  # predefined query\n",
        "  query_dict={'Manolo Jiménez Salinas':'\"Manolo Jiménez Salinas\" OR manolojim OR manolojimenezs OR Manolo.Jimenez.Salinas',\n",
        "      'Armando Guadiana Tijerina':'\"Armando Guadiana Tijerina\" OR aguadiana OR armandoguadianatijerina OR ArmandoGuadianaTijerina',\n",
        "'Ricardo Mejia Berdeja':'\"Ricardo Mejia Berdeja\" OR RicardoMeb OR ricardomeb OR RicardoMejiaMx',\n",
        "    'Lenin Perez Rivera':'\"Lenin Perez Rivera\" OR leninperezr OR leninperezr OR leninperezr'\n",
        "}\n",
        "  # information tracer token\n",
        "  with open(os.path.expanduser('infotracer_token.txt'), 'r') as f:\n",
        "      your_token = f.read()\n",
        "\n",
        "  # query with information tracer api\n",
        "  df=[]\n",
        "\n",
        "  for candidate, query in query_dict.items():\n",
        "    id_hash256 = informationtracer.trace(query=query, token=your_token, start_date=start_date, end_date=end_date)\n",
        "    url = \"https://informationtracer.com/api/v1/result?token={}&id_hash256={}\".format(your_token, id_hash256)\n",
        "    results = requests.get(url).json() #will get json for all data of keyword, results is a dictionary\n",
        "\n",
        "    for pf in ['facebook','twitter','instagram','youtube']:\n",
        "      platform_data=pd.DataFrame(results['posts'][pf], columns=['d','i','n','t'])\n",
        "      platform_data['platform']=pf\n",
        "      platform_data['candidate_name']=candidate\n",
        "      platform_data['t']=pd.to_datetime(platform_data['t'])\n",
        "      df.append(platform_data)\n",
        "\n",
        "  df=pd.concat(df)\n",
        "  df=df.rename(columns={'d':'text','i':'num_interaction','n':'username','t':'datetime'})\n",
        "\n",
        "  # convert timezone\n",
        "  df['datetime'] = convert_time(df['datetime'])\n",
        "  df=df.drop_duplicates()\n",
        "\n",
        "  print('#################################################')\n",
        "  print('the shape of infotracer table is:',df.shape)\n",
        "  print('#################################################')\n",
        "\n",
        "  if update_db==True:\n",
        "    # insert to db\n",
        "    infotracer_data = df.apply(tuple, axis=1).tolist()\n",
        "    query=\"insert into infotracer (text,num_interaction,username,datetime,platform,candidate_name) Values(%s,%s,%s,%s,%s,%s);\"\n",
        "    mycursor.executemany(query,infotracer_data)\n",
        "\n",
        "    mydb.commit()\n",
        "    # return query result\n",
        "  return df"
      ],
      "metadata": {
        "id": "sASFpEI-4xUv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result416=generate_infotracer_table(start_date='2023-04-16',end_date='2023-04-17',update_db=False)"
      ],
      "metadata": {
        "id": "PFBqyu5EFUmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL data for sentiment table"
      ],
      "metadata": {
        "id": "gdcmvmfVWmrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## youtube comment functions"
      ],
      "metadata": {
        "id": "fOVZ02cygZN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "##  NOTE: register youtube API key!\n",
        "## \n",
        "##  Follow instruction here: \n",
        "##  https://developers.google.com/youtube/registering_an_application\n",
        "##\n",
        "##  One key is enough, more keys can speed up the data collection process\n",
        "#############################################################################\n",
        "\n",
        "# read a dictionary of names and values of youtube api key \n",
        "API_KEY= {}\n",
        "\n",
        "with open(os.path.expanduser('youtube_tokens.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        key, value = line.strip().split(',')\n",
        "        API_KEY[key] = value\n",
        "\n",
        "API_KEY = list(API_KEY.values())\n",
        "\n",
        "def search_replies(comment_id):\n",
        "    headers = {\n",
        "        'Accept': 'application/json',\n",
        "    }    \n",
        "\n",
        "    params = (\n",
        "        ('part', 'snippet,id'),\n",
        "        ('parentId', comment_id),\n",
        "        ('key', random.choice(API_KEY)),\n",
        "        ('maxResults', '100')        \n",
        "    )\n",
        "    \n",
        "    results = []\n",
        "    response = requests.get('https://www.googleapis.com/youtube/v3/comments', headers=headers, params=params)\n",
        "    \n",
        "    print(\"Youtube response code --> \", response.status_code)\n",
        "    if int(response.status_code) > 300:\n",
        "        print(response.text)\n",
        "        print('Youtube API v3/videos returns non 200 status code, something is very wrong')\n",
        "        return results\n",
        "\n",
        "    results += [item for item in response.json()['items']]\n",
        "#     print(results)\n",
        "    nextPageToken = response.json().get('nextPageToken', None)\n",
        "    \n",
        "    while nextPageToken:\n",
        "        print(nextPageToken)\n",
        "        params = (\n",
        "            ('part', 'snippet,id'),\n",
        "            ('parentId', comment_id),\n",
        "            ('key', random.choice(API_KEY)),\n",
        "            ('maxResults', '100'),\n",
        "            ('pageToken', nextPageToken)\n",
        "        )\n",
        "        \n",
        "        response = requests.get('https://www.googleapis.com/youtube/v3/comments', headers=headers, params=params)\n",
        "        print(\"Youtube response code --> \", response.status_code)\n",
        "        if int(response.status_code) > 300:\n",
        "            print(response.text)\n",
        "            print('Youtube API v3/videos returns non 200 status code, something is very wrong')\n",
        "            break\n",
        "    \n",
        "        results += [item for item in response.json()['items']]\n",
        "        nextPageToken = response.json().get('nextPageToken', None)\n",
        "        \n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def search_comments(video_id):        \n",
        "    headers = {\n",
        "        'Accept': 'application/json',\n",
        "    }    \n",
        "\n",
        "    params = (\n",
        "        ('part', 'snippet,replies'),\n",
        "        ('videoId', video_id),\n",
        "        ('key', random.choice(API_KEY)),\n",
        "        ('maxResults', '100')        \n",
        "    )\n",
        "    \n",
        "    results = []\n",
        "    response = requests.get('https://www.googleapis.com/youtube/v3/commentThreads', headers=headers, params=params)\n",
        "    print(\"Youtube response code --> \", response.status_code)\n",
        "    if int(response.status_code) > 300:\n",
        "        print(response.text)\n",
        "        print('Youtube API v3/videos returns non 200 status code, something is very wrong')\n",
        "        return results\n",
        "\n",
        "    results += [item for item in response.json()['items']]\n",
        "    nextPageToken = response.json().get('nextPageToken', None)\n",
        "    \n",
        "    while nextPageToken:\n",
        "        print(nextPageToken)\n",
        "        params = (\n",
        "            ('part', 'snippet,replies'),\n",
        "            ('videoId', video_id),\n",
        "            ('key', random.choice(API_KEY)),\n",
        "            ('maxResults', '100'),\n",
        "            ('pageToken', nextPageToken)\n",
        "        )\n",
        "        \n",
        "        response = requests.get('https://www.googleapis.com/youtube/v3/commentThreads', headers=headers, params=params)\n",
        "        print(\"Youtube response code --> \", response.status_code)\n",
        "        if int(response.status_code) > 300:\n",
        "            print(response.text)\n",
        "            print('Youtube API v3/videos returns non 200 status code, something is very wrong')\n",
        "            break\n",
        "    \n",
        "        results += [item for item in response.json()['items']]\n",
        "        nextPageToken = response.json().get('nextPageToken', None)\n",
        "    \n",
        "    for i in results:\n",
        "        if i['snippet']['totalReplyCount'] > 5:\n",
        "            print('hydrating more comments...')\n",
        "            i['replies']['comments'] = search_replies(i['id'])\n",
        "            print(len(i['replies']['comments']))\n",
        "\n",
        "    # save results    \n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "gAH6Cj6ShhSC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_time_ytb(column):\n",
        "##############################################################################\n",
        "##  This function takes the datetime column of any dataframe \n",
        "##  and convert utc time to mexico time. For ytb comment only.\n",
        "##  This function is part of another function. DO NOT run directly.\n",
        "##############################################################################\n",
        "\n",
        "  #utc to mexico time\n",
        "  mexico_tz = timezone('America/Mexico_City')\n",
        "  convert_timestamp = lambda x: x.astimezone(mexico_tz).replace(tzinfo=None)\n",
        "  column = column.apply(convert_timestamp)\n",
        "  return column\n"
      ],
      "metadata": {
        "id": "rV2HnRbHDgDN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## youtube query"
      ],
      "metadata": {
        "id": "QlJW76Jw1zOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def query_youtube_comment(start_date,end_date):\n",
        "##############################################################################\n",
        "##  This function use information tracer result to query for youtube comment, \n",
        "##  and convert utc time to mexico time.\n",
        "##  This function is part of another function. DO NOT run directly.\n",
        "##############################################################################\n",
        "  # predefined query\n",
        "  query_dict={'Manolo Jiménez Salinas':'\"Manolo Jiménez Salinas\" OR manolojim OR manolojimenezs OR Manolo.Jimenez.Salinas',\n",
        "      'Armando Guadiana Tijerina':'\"Armando Guadiana Tijerina\" OR aguadiana OR armandoguadianatijerina OR ArmandoGuadianaTijerina',\n",
        "'Ricardo Mejia Berdeja':'\"Ricardo Mejia Berdeja\" OR RicardoMeb OR ricardomeb OR RicardoMejiaMx',\n",
        "    'Lenin Perez Rivera':'\"Lenin Perez Rivera\" OR leninperezr OR leninperezr OR leninperezr'\n",
        "}\n",
        "  # information tracer token\n",
        "  with open(os.path.expanduser('infotracer_token.txt'), 'r') as f:\n",
        "      your_token = f.read()\n",
        "\n",
        "  # get video id from information tracer source data\n",
        "  videoId_dic={}\n",
        "  for candidate, query in query_dict.items():\n",
        "    id_hash256 = informationtracer.trace(query=query, token=your_token, start_date=start_date, end_date=end_date,skip_result=True)\n",
        "    url=\"https://informationtracer.com/loadsource?source={}&id_hash256={}&token={}\".format('youtube', id_hash256, your_token)\n",
        "    results=requests.get(url).json()\n",
        "    videoId_dic[candidate] = [data['id']['videoId'] for data in results] \n",
        "  \n",
        "  # query comments using youtube api\n",
        "  ytbcomment_df=[]\n",
        "  for candidate, videoId_list in videoId_dic.items():\n",
        "    comment=[]\n",
        "    username=[]\n",
        "    likeCount=[]\n",
        "    date=[]\n",
        "    for vid in videoId_list:\n",
        "      if vid!=[]:\n",
        "        result=search_comments(vid)\n",
        "        for i in np.arange(0,len(result),1):\n",
        "          comment.append(result[i]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
        "          username.append(result[i]['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
        "          likeCount.append(result[i]['snippet']['topLevelComment']['snippet']['likeCount'])\n",
        "          date.append(result[i]['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
        "      ytbcomment= pd.DataFrame({'text': comment, 'num_interaction': likeCount, \n",
        "                      'username': username,'datetime': date})\n",
        "      ytbcomment['platform']='youtube comment'\n",
        "      ytbcomment['candidate_name']=candidate\n",
        "      ytbcomment['datetime']=pd.to_datetime(ytbcomment['datetime'])\n",
        "      ytbcomment_df.append(ytbcomment)\n",
        "  ytbcomment_df=pd.concat(ytbcomment_df)\n",
        "  \n",
        "\n",
        "  # convert timezone\n",
        "  if ytbcomment_df.empty==False:\n",
        "    ytbcomment_df['datetime'] = convert_time_ytb(ytbcomment_df['datetime'])\n",
        "    ytbcomment_df=ytbcomment_df.drop_duplicates()\n",
        "\n",
        "  print('#################################################')\n",
        "  print('the shape of youtube comment is:',ytbcomment_df.shape)\n",
        "  print('#################################################')\n",
        "  # return query result\n",
        "  return ytbcomment_df"
      ],
      "metadata": {
        "id": "YPsQS-HFvzlf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text process functions"
      ],
      "metadata": {
        "id": "DFqANjUdFrp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove \\n and ...\n",
        "def remove_n(df):\n",
        "  df['text'] = df['text'].str.replace(r'\\n|\\n.', '', regex=True)\n",
        "  df['text'] = df['text'].str.replace(r'\\.{2,}', '.', regex=True)\n",
        "  return df\n",
        "  \n",
        "# parse\n",
        "def parse(df):\n",
        "  print('#################################################')\n",
        "  print('the shape of merged df is:',df.shape)\n",
        "  print('start parsing')\n",
        "  print('#################################################')\n",
        "  df=remove_n(df)\n",
        "  df=df.drop_duplicates()\n",
        "  # new_df = pd.DataFrame(columns=df.columns)\n",
        "  parsed_df=[]\n",
        "  # iterate over each row in the original dataframe\n",
        "  for index, row in df.iterrows():\n",
        "    print('parsing row', index)\n",
        "    # split the \"text\" column value into a list of sentences\n",
        "    sentences = row['text'].split('. ')  # assuming sentences are separated by \". \"\n",
        "    new_df=pd.DataFrame({\n",
        "            'text': sentences,\n",
        "            'num_interaction': row['num_interaction'],\n",
        "            'username':row['username'],\n",
        "            'datetime': row['datetime'],\n",
        "            'platform': row['platform'],\n",
        "            'candidate_name': row['candidate_name']            \n",
        "    })\n",
        "    parsed_df.append(new_df)\n",
        "\n",
        "  parsed_df=pd.concat(parsed_df)\n",
        "  return parsed_df"
      ],
      "metadata": {
        "id": "gZzBlKjjFp3z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to restore spanish accents\n",
        "def replace_special_chars(text):\n",
        "  text = re.sub(r'\\\\u([\\da-fA-F]{4})', lambda m: chr(int(m.group(1), 16)), text)\n",
        "  # Remove punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  text = ''.join([i for i in text if not i.isdigit()])\n",
        "  # # Create a SnowballStemmer for Spanish\n",
        "  # stemmer = SnowballStemmer('spanish')\n",
        "  # # Apply stemming\n",
        "  # words = text.split()\n",
        "  # stemmed_words = [stemmer.stem(word) for word in words]\n",
        "  # return ' '.join(stemmed_words)\n",
        "  return text\n",
        "\n",
        "def text_process(df):\n",
        "  # restore accent\n",
        "  df['text'] = df['text'].apply(replace_special_chars)\n",
        "  # process tweet is a func from pysentimiento\n",
        "  df['processed_text'] = df['text'].apply(preprocess_tweet)\n",
        "\n",
        "  # other steps\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "sjWZWxqlxR0a"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sentiment analysis function"
      ],
      "metadata": {
        "id": "ehXhPtXYMu3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do sentiment analysis in batches\n",
        "def sent_analyze(df):\n",
        "  batch=512\n",
        "  analyzer = create_analyzer(task=\"sentiment\", lang=\"es\", batch_size=batch)\n",
        "\n",
        "  text=df['processed_text'].tolist() #list of texts\n",
        "  label=[]\n",
        "  pos_prob=[]\n",
        "  neu_prob=[]\n",
        "  neg_prob=[]\n",
        "  for i in range(0, len(text), batch):\n",
        "    analyze_result=analyzer.predict(text[i:i+batch])\n",
        "\n",
        "    label+=[r.output for r in analyze_result]\n",
        "    pos_prob+=[r.probas['POS'] for r in analyze_result]\n",
        "    neu_prob+=[r.probas['NEU'] for r in analyze_result]\n",
        "    neg_prob+=[r.probas['NEG'] for r in analyze_result]\n",
        "    print(\"Batch {} done\".format(int(i/512)))\n",
        "\n",
        "  df['label']=label\n",
        "  df['positive']=pos_prob\n",
        "  df['neutral']=neu_prob\n",
        "  df['negative']=neg_prob\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "bxFlp_AFxR2x"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_infotracer_and_sentiment_table(start_date,end_date,ytb_end_date,update_db=True):\n",
        "\n",
        "###########################################################################\n",
        "## This function generates both infotracer and sentiment table.\n",
        "## It calls information tracer and ytb comment function within it,\n",
        "## merge result, process text and conduct sentiment analysis.\n",
        "###########################################################################\n",
        "\n",
        "  # predefined query\n",
        "  query_dict={'Manolo Jiménez Salinas':'\"Manolo Jiménez Salinas\" OR manolojim OR manolojimenezs OR Manolo.Jimenez.Salinas',\n",
        "      'Armando Guadiana Tijerina':'\"Armando Guadiana Tijerina\" OR aguadiana OR armandoguadianatijerina OR ArmandoGuadianaTijerina',\n",
        "'Ricardo Mejia Berdeja':'\"Ricardo Mejia Berdeja\" OR RicardoMeb OR ricardomeb OR RicardoMejiaMx',\n",
        "    'Lenin Perez Rivera':'\"Lenin Perez Rivera\" OR leninperezr OR leninperezr OR leninperezr'\n",
        "}\n",
        "  # information tracer token\n",
        "  with open(os.path.expanduser('infotracer_token.txt'), 'r') as f:\n",
        "      your_token = f.read()\n",
        "\n",
        "  # get information tracer query result (also insert to db)\n",
        "  df=generate_infotracer_table(start_date=start_date,end_date=end_date,update_db=update_db)\n",
        "\n",
        "  #get youtube comment query result\n",
        "  ytbcomment_df=query_youtube_comment(start_date=start_date,end_date=ytb_end_date)\n",
        "\n",
        "  # merge results\n",
        "  # text for sentiment table: everything in df + used youtube raw data to extract comment\n",
        "  sentiment_df=pd.concat([ytbcomment_df,df])\n",
        "  sentiment_df=sentiment_df.reset_index(drop=True)\n",
        "\n",
        "  #parse post into sentences\n",
        "  sentiment=parse(sentiment_df)\n",
        "\n",
        "  #################################\n",
        "  ## text processing\n",
        "  #################################\n",
        "  sentiment=text_process(sentiment)\n",
        "\n",
        "  # sentiment calculation\n",
        "\n",
        "  full_sentiment=sent_analyze(sentiment)\n",
        "  if update_db==True:\n",
        "    # commit full sentiment to sentiment table in db\n",
        "    sent_data = full_sentiment.apply(tuple, axis=1).tolist()\n",
        "    query=\"insert into sentiment (text,num_interaction,username,datetime,platform,candidate_name,processed_text,label, positive, neutral, negative) Values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\" \n",
        "    mycursor.executemany(query,sent_data)\n",
        "\n",
        "    mydb.commit()\n",
        "\n",
        "  return \n"
      ],
      "metadata": {
        "id": "NXk46zTP5MYn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL data for wordcloud\n"
      ],
      "metadata": {
        "id": "lIi2F_tB1YAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(sentence):\n",
        "  # all lower-case\n",
        "  sentence = sentence.lower()\n",
        "  # remove punctuations \n",
        "  sentence = re.sub(r\"[,.¡¿\\\"@#$%^&*(){}?/;:<>+=-]\", \"\", sentence)\n",
        "  # remove numbers\n",
        "  sentence = re.sub(r'\\d+', '', sentence)\n",
        "  # remove leading/trailing whitespace\n",
        "  sentence = sentence.strip()\n",
        "  \n",
        "  return sentence\n"
      ],
      "metadata": {
        "id": "t_8_RG2o4c5x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_wordcloud_table():\n",
        "  #query for data in last 14 days\n",
        "  wc_query= \"SELECT processed_text, platform, candidate_name, datetime FROM sentiment WHERE datetime >= DATE_SUB(NOW(), INTERVAL 14 DAY) \"\n",
        "  word_cloud_df= pd.read_sql_query(wc_query, mydb)\n",
        "  \n",
        "  #process text\n",
        "  word_cloud_df['processed_text']=word_cloud_df['processed_text'].apply(lambda x: clean_text(x))\n",
        "\n",
        "\n",
        "  # stop words\n",
        "  \n",
        "  with open(os.path.expanduser('spanish.txt'), 'r') as f:\n",
        "    spanish_stopwords = [line.strip() for line in f]\n",
        "  stop_words = set(spanish_stopwords)\n",
        "  stop_words.add('emoji') \n",
        "  stop_words.add('http')\n",
        "  stop_words.add('youtube')\n",
        "  stop_words.add('twitter')\n",
        "  stop_words.add('instagram')\n",
        "  stop_words.add('facebook')\n",
        "\n",
        "  # generate wordcloud df\n",
        "  wc=[]\n",
        "  for platform in word_cloud_df['platform'].unique():\n",
        "    for candidate in word_cloud_df['candidate_name'].unique():\n",
        "      all_text = ' '.join(word_cloud_df[(word_cloud_df['platform']==platform)&(word_cloud_df['candidate_name']==candidate)]['processed_text'])\n",
        "      if all_text=='':\n",
        "        all_text='NADA'\n",
        "        print(platform, candidate, all_text)\n",
        "      wordcloud = WordCloud(stopwords=stop_words).generate(all_text)\n",
        "      # Get list of words in wordcloud\n",
        "      word_list = list(wordcloud.words_.keys())\n",
        "      word_weights = list(wordcloud.words_.values())\n",
        "      sub_wc=pd.DataFrame({'word':word_list, 'frequency':word_weights,'platform': platform,'candidate_name':candidate})\n",
        "      wc.append(sub_wc)\n",
        "\n",
        "  wc=pd.concat(wc)\n",
        "  print('shape of wordcloud is',wc.shape)\n",
        "\n",
        "  # empty the table\n",
        "  print('empty the table')\n",
        "  mycursor.execute(f'DELETE FROM wordcloud')\n",
        "  mydb.commit()\n",
        "\n",
        "  # insert to db last 14 days data\n",
        "  print('start inserting')\n",
        "  wc_data = wc.apply(tuple, axis=1).tolist()\n",
        "  query=\"insert into wordcloud (word,weight, platform, candidate_name) Values(%s,%s,%s,%s);\" \n",
        "  mycursor.executemany(query,wc_data)\n",
        "  print('done')\n",
        "\n",
        "  mydb.commit()\n",
        "\n",
        "  return \n"
      ],
      "metadata": {
        "id": "qQLPfpp_dgau"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL data for network"
      ],
      "metadata": {
        "id": "1ey2PPKgRU1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_network_table(start_date,end_date,update_db=True):\n",
        "  \n",
        "  # predefined query\n",
        "  query_dict={'Manolo Jiménez Salinas':'\"Manolo Jiménez Salinas\" OR manolojim OR manolojimenezs OR Manolo.Jimenez.Salinas',\n",
        "      'Armando Guadiana Tijerina':'\"Armando Guadiana Tijerina\" OR aguadiana OR armandoguadianatijerina OR ArmandoGuadianaTijerina',\n",
        "'Ricardo Mejia Berdeja':'\"Ricardo Mejia Berdeja\" OR RicardoMeb OR ricardomeb OR RicardoMejiaMx',\n",
        "    'Lenin Perez Rivera':'\"Lenin Perez Rivera\" OR leninperezr OR leninperezr OR leninperezr'\n",
        "}\n",
        "\n",
        "  # information tracer token\n",
        "  with open(os.path.expanduser('infotracer_token.txt'), 'r') as f:\n",
        "      your_token = f.read()\n",
        "\n",
        "  for candidate, query in query_dict.items():\n",
        "    ## extract data\n",
        "    id_hash256 = informationtracer.trace(query=query, token=your_token, start_date=start_date, end_date=end_date)\n",
        "    url = \"https://informationtracer.com/cross_platform/{}/interaction_network_{}.json\".format(id_hash256[:3], id_hash256)\n",
        "\n",
        "    network_json = requests.get(url).json()\n",
        "\n",
        "    ## construct df\n",
        "    node_id_to_category = {}\n",
        "    node_id_to_name={}\n",
        "    node_id_to_datetime={}\n",
        "    node_id_to_platform={}\n",
        "\n",
        "    ## iterate all nodes to save node type\n",
        "    for node in network_json['nodes']:\n",
        "      node_id_to_category[node['id']] = node['type']\n",
        "      node_id_to_name[node['id']]=node['name']\n",
        "      node_id_to_datetime[node['id']]=node['timestamp']\n",
        "      node_id_to_platform[node['id']]=node['platform']\n",
        "\n",
        "    ## iterate all links\n",
        "    source=[]\n",
        "    target=[]\n",
        "    weight=[]\n",
        "    source_cat=[]\n",
        "    target_cat=[]\n",
        "    source_datetime=[]\n",
        "    target_datetime=[]\n",
        "\n",
        "    for link in network_json['links']:\n",
        "      source_id = link['source']\n",
        "      target_id = link['target']\n",
        "      source_datetime.append(node_id_to_datetime[source_id])\n",
        "      target_datetime.append(node_id_to_datetime[target_id])\n",
        "\n",
        "      source_cat.append(node_id_to_category[source_id])\n",
        "      target_cat.append(node_id_to_category[target_id])\n",
        "\n",
        "      # to distinguish user with same name on different platform\n",
        "      if node_id_to_category[source_id]=='user':\n",
        "        source.append((node_id_to_name[source_id]+' '+node_id_to_platform[source_id]))\n",
        "      else: \n",
        "        source.append(node_id_to_name[source_id])\n",
        "      \n",
        "      if node_id_to_category[target_id]=='user':\n",
        "        target.append((node_id_to_name[target_id]+' '+node_id_to_platform[target_id]))\n",
        "      else:\n",
        "        target.append(node_id_to_name[target_id])\n",
        "      \n",
        "      weight.append(link['weight'])\n",
        "\n",
        "\n",
        "    nw_df = pd.DataFrame({\n",
        "        'source': source,\n",
        "        'target': target,\n",
        "        'weight': weight,\n",
        "        'source_cat': source_cat,\n",
        "        'target_cat': target_cat,\n",
        "        'source_datetime': source_datetime,\n",
        "        'target_datetime': target_datetime\n",
        "    })\n",
        "    nw_df['source_datetime']=pd.to_datetime(nw_df['source_datetime'])\n",
        "    nw_df['target_datetime']=pd.to_datetime(nw_df['target_datetime'])\n",
        "    nw_df['source_datetime']=convert_time(nw_df['source_datetime'])\n",
        "    nw_df['target_datetime']=convert_time(nw_df['target_datetime'])\n",
        "\n",
        "    nw_df['candidate_name']=candidate\n",
        "    if update_db==True:\n",
        "      ##### insert into mysql table source, target, weight, source_category, target_category\n",
        "      nw_data=nw_df.apply(tuple, axis=1).tolist()\n",
        "      query=\"insert into network (source, target, weight, source_cat, target_cat, source_datetime, target_datetime, candidate_name) Values(%s,%s,%s,%s,%s,%s,%s,%s);\"\n",
        "      mycursor.executemany(query,nw_data)\n",
        "\n",
        "      mydb.commit()\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "9TktXSHCUnc9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DF1dtrBhqvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Historical"
      ],
      "metadata": {
        "id": "KerIZsScjrqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if start date=0415, end date=0416, it means from 0415 00:00 to 0416 23:59!!!\n",
        "# 0415 00：00-0416-23：59 << start 0415, end 0416\n",
        "# 0416 00：00-0416 23：59 << start 0416, end 0416"
      ],
      "metadata": {
        "id": "-mMyrDTUPBZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # infotracer and sentiment: add historical data\n",
        "# generate_infotracer_and_sentiment_table(start_date=\"2023-01-01\",end_date=\"2023-04-08\",ytb_end_date=\"2023-04-08\")\n",
        "# # network: add historical data\n",
        "# generate_network_table(start_date=\"2023-01-01\",end_date=\"2023-04-08\")\n",
        "# # helper: add historical data\n",
        "# generate_helper_table(start_date=\"2023-01-01\",end_date=\"2023-04-08\")"
      ],
      "metadata": {
        "id": "hdeyjJvD8fo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Daily update"
      ],
      "metadata": {
        "id": "ucQRvPHGjtv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_dict={'Manolo Jiménez Salinas':'\"Manolo Jiménez Salinas\" OR manolojim OR manolojimenezs OR Manolo.Jimenez.Salinas',\n",
        "      'Armando Guadiana Tijerina':'\"Armando Guadiana Tijerina\" OR aguadiana OR armandoguadianatijerina OR ArmandoGuadianaTijerina',\n",
        "'Ricardo Mejia Berdeja':'\"Ricardo Mejia Berdeja\" OR RicardoMeb OR ricardomeb OR RicardoMejiaMx',\n",
        "    'Lenin Perez Rivera':'\"Lenin Perez Rivera\" OR leninperezr OR leninperezr OR leninperezr'\n",
        "}"
      ],
      "metadata": {
        "id": "q5CqOqZ43HST"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('##############################')\n",
        "print('Update starts')\n",
        "\n",
        "# helper: daily update\n",
        "# print('helper table starts')\n",
        "# today = datetime.now(pytz.timezone('America/Mexico_City')).date()\n",
        "# tomorrow=datetime.now(pytz.timezone('America/Mexico_City')).date() + timedelta(days=1)\n",
        "# generate_helper_table(start_date=tomorrow, end_date=tomorrow, update_db=True)\n",
        "# print('helper table done')\n",
        "\n",
        "\n",
        "# infotracer and sentiment: daily update\n",
        "print('infotracer and sentiment table start')\n",
        "# today = datetime.now(pytz.timezone('America/Mexico_City')).date().strftime('%Y-%m-%d')\n",
        "# tomorrow=(datetime.now(pytz.timezone('America/Mexico_City')).date() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "today='2023-04-16'\n",
        "tomorrow='2023-04-17'\n",
        "generate_infotracer_and_sentiment_table(start_date=today, end_date=tomorrow, ytb_end_date=tomorrow,update_db=True)\n",
        "print('infotracer and sentiment table done')\n",
        "\n",
        "# wordcloud: daily update\n",
        "print('wordcloud table starts')\n",
        "generate_wordcloud_table()\n",
        "print('wordcloud table done')\n",
        "\n",
        "# network: daily update\n",
        "print('network table starts')\n",
        "generate_network_table(start_date=today, end_date=tomorrow,update_db=True)\n",
        "print('network table done')\n",
        "\n",
        "print('Update ends')\n",
        "print('##############################')\n"
      ],
      "metadata": {
        "id": "MEMO5aq5LmMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('network table starts')\n",
        "generate_network_table(start_date='2023-04-16', end_date='2023-04-20',update_db=False)\n",
        "print('network table done')"
      ],
      "metadata": {
        "id": "SYndumf3WrhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LllxHMWSDDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}